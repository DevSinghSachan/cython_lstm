{
 "metadata": {
  "name": "",
  "signature": "sha256:52882a32b0e813869b3d596ed3979f7f5672d3f4f91ce3f4ce2d223163e788a4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "% load_ext autoreload\n",
      "% autoreload 2\n",
      "% matplotlib inline\n",
      "% load_ext cythonmagic\n",
      "% config InlineBackend.figure_format = 'svg'\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np, matplotlib\n",
      "\n",
      "from cython_lstm.network import Network\n",
      "from cython_lstm.neuron  import Neuron, LogisticNeuron, TanhNeuron, SoftmaxNeuron\n",
      "from cython_lstm.layers  import Layer, TileLayer, TemporalLayer, RecurrentLayer, RecurrentAveragingLayer, RecurrentGatedLayer, RecurrentMultiStageLayer\n",
      "from cython_lstm.trainer import Trainer\n",
      "from cython_lstm.dataset import create_xor_dataset, create_digit_dataset\n",
      "\n",
      "SIZE = 10\n",
      "INTERNAL_SIZE = 5\n",
      "TIMESTEPS = 2\n",
      "STREAMS = 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Testing recurrent behavior\n",
      "\n",
      "## Forward propagation\n",
      "\n",
      "Forward propagation through time is simple, and should be efficient memory wise. It requires some planning at the wiring stage to know when the output of one stage is ready for the next. Here we test this assumption by constructing two very similar networks.\n",
      "\n",
      "Both have an input layer, a gate, and a layer that averages the previous activation with the new one using the gate. In the first network the gate is fed by the input, in the second network, the gate is fed by the activation of the first layer.\n",
      "\n",
      "Here we perform both sets of calculations using an internal for loop that does book keeping for us, and we also perform the same calculation using an explicit loop for inspectability. Both operations should return the same result, regardless of the network wiring."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net1 = Network()\n",
      "\n",
      "# create the layers\n",
      "linear_layer    = TemporalLayer(SIZE, INTERNAL_SIZE, neuron = TanhNeuron)\n",
      "gate            = RecurrentGatedLayer(SIZE, neuron = LogisticNeuron)\n",
      "averaging_layer = RecurrentAveragingLayer(gate, linear_layer)\n",
      "\n",
      "# input should be dispatched to the gate and the linear layer\n",
      "tiles = TileLayer()\n",
      "tiles.connect_to(gate, temporal=True)\n",
      "tiles.connect_to(linear_layer, temporal=True)\n",
      "\n",
      "linear_layer.connect_to(averaging_layer, temporal=True)\n",
      "\n",
      "averaging_layer._initial_hidden_state += np.random.standard_normal(averaging_layer._initial_hidden_state.shape)\n",
      "\n",
      "net1.add_layer(tiles, input=True)\n",
      "net1.add_layer(linear_layer)\n",
      "net1.add_layer(gate)\n",
      "net1.add_layer(averaging_layer, output=True)\n",
      "\n",
      "recurrent_data = np.random.standard_normal([TIMESTEPS, STREAMS, SIZE]).astype(np.float32)\n",
      "out = net1.activate(recurrent_data)[-1]\n",
      "net1.clear()\n",
      "\n",
      "# manual pass\n",
      "\n",
      "net1.layers[1].allocate_activation(TIMESTEPS, STREAMS)\n",
      "net1.layers[2].allocate_activation(TIMESTEPS, STREAMS)\n",
      "net1.layers[3].allocate_activation(TIMESTEPS, STREAMS)\n",
      "\n",
      "for t in range(TIMESTEPS):\n",
      "    out1 = net1.layers[1].forward_propagate(recurrent_data[t, :, :])\n",
      "    out2 = net1.layers[2].forward_propagate(recurrent_data[t, :, :])\n",
      "    out3 = net1.layers[3].forward_propagate(out1)\n",
      "    net1.layers[1].step += 1\n",
      "    net1.layers[2].step += 1\n",
      "    net1.layers[3].step += 1\n",
      "net.clear()\n",
      "\n",
      "# comparison\n",
      "print(\"Outputs are identical => \", np.allclose(out, out3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Outputs are identical =>  True\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alternate wiring diagram, now the output of the first Layer feeds the gate:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net2 = Network()\n",
      "\n",
      "# create the layers\n",
      "recurrent_layer = TemporalLayer(SIZE, INTERNAL_SIZE, neuron = TanhNeuron)\n",
      "gate            = RecurrentGatedLayer(INTERNAL_SIZE, neuron = LogisticNeuron)\n",
      "averaging_layer = RecurrentAveragingLayer(gate, recurrent_layer)\n",
      "\n",
      "# connect them in the order the computation should proceed\n",
      "recurrent_layer.connect_to(gate, temporal=True)\n",
      "recurrent_layer.connect_to(averaging_layer, temporal=True)\n",
      "\n",
      "#averaging_layer.connect_to(recurrent_layer, temporal=True)\n",
      "\n",
      "averaging_layer._initial_hidden_state += np.random.standard_normal(averaging_layer._initial_hidden_state.shape)\n",
      "\n",
      "net2.add_layer(recurrent_layer, input=True)\n",
      "net2.add_layer(gate)\n",
      "net2.add_layer(averaging_layer, output=True)\n",
      "\n",
      "recurrent_data = np.random.standard_normal([TIMESTEPS, STREAMS, SIZE]).astype(np.float32)\n",
      "out = net2.activate(recurrent_data)[-1]\n",
      "net2.clear()\n",
      "\n",
      "net2.layers[0].allocate_activation(TIMESTEPS, STREAMS)\n",
      "net2.layers[1].allocate_activation(TIMESTEPS, STREAMS)\n",
      "net2.layers[2].allocate_activation(TIMESTEPS, STREAMS)\n",
      "\n",
      "for t in range(TIMESTEPS):\n",
      "    out1 = net2.layers[0].forward_propagate(recurrent_data[t, :, :])\n",
      "    out2 = net2.layers[1].forward_propagate(out1)\n",
      "    out3 = net2.layers[2].forward_propagate(out1)\n",
      "    net2.layers[0].step += 1\n",
      "    net2.layers[1].step += 1\n",
      "    net2.layers[2].step += 1\n",
      "net2.clear()\n",
      "print(\"Outputs are identical => \", np.allclose(out, out3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Outputs are identical =>  True\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Backward propagation\n",
      "\n",
      "Backward propagation in general can be done nicely for well designed graphs. However whenever cycles are introduced we need to perform backpropagation through time. In this instance we want to make sure these operations are well defined, and that the internal bookkeeping is done correctly, so that the error signal is sent through all stages of the computational graph correctly.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}