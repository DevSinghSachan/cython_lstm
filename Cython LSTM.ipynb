{
 "metadata": {
  "name": "",
  "signature": "sha256:968d05372a09243f8b4a15a4cd33c610a0524b39da0a3c7281c417d8a5e02e90"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "% load_ext autoreload\n",
      "% autoreload 2\n",
      "% matplotlib inline\n",
      "%load_ext cythonmagic\n",
      "%config InlineBackend.figure_format = 'svg'\n",
      "import numpy as np\n",
      "REAL = np.float32\n",
      "from cython_lstm import vector_outer_product"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(x):\n",
      "    return 1. / (1. + np.exp(-x))\n",
      "\n",
      "class Network():\n",
      "    \"\"\"\n",
      "    Create a simple neural network\n",
      "    \"\"\"\n",
      "    def __init__(self):\n",
      "        self._layers = []\n",
      "        self._output_layer = None\n",
      "        self._input_layer = None\n",
      "        self._data_layer = DataLayer()\n",
      "        \n",
      "        \n",
      "    def add_layer(self, layer, input=False, output = False):\n",
      "        self._layers.append(layer)\n",
      "        if input:\n",
      "            self._input_layer = layer\n",
      "            # connect the data layer to the first network\n",
      "            # layer:\n",
      "            self._data_layer._forward_layer = layer\n",
      "            layer.add_backward_layer(self._data_layer)\n",
      "        if output:\n",
      "            self._output_layer = layer\n",
      "        \n",
      "    def remove_layer(self, layer):\n",
      "        self._layers.remove(layer)\n",
      "        \n",
      "    def activate(self, input):\n",
      "        self._data_layer.activate(input)\n",
      "        \n",
      "    def backpropagate(self, target):\n",
      "        if target.ndim == 1:\n",
      "            target = target.reshape(1,-1)\n",
      "        self._output_layer.error_activate(target)\n",
      "        \n",
      "    def clear(self):\n",
      "        \"\"\"\n",
      "        Resets the state of the layers in this\n",
      "        neural network\n",
      "        \"\"\"\n",
      "        for layer in self._layers:\n",
      "            layer.clear()\n",
      "        \n",
      "    def __repr__(self):\n",
      "        return str({\n",
      "    \"layers\": self._layers,\n",
      "    \"output_layer\": self._output_layer,\n",
      "    \"input_layer\": self._input_layer\n",
      "        })\n",
      "    \n",
      "class DataLayer():\n",
      "    def __init__(self):\n",
      "        self._activation = None\n",
      "        self._forward_layer = None\n",
      "        \n",
      "    def activate(self, input):\n",
      "        if input.ndim == 1:\n",
      "            self._activation = input.reshape(1,-1)\n",
      "        else:\n",
      "            self._activation = input\n",
      "        self._forward_layer.activate(input)\n",
      "        \n",
      "    def backpropagate(self, signal):\n",
      "        \"\"\"\n",
      "        Backpropagating through data layer\n",
      "        is not useful here.\n",
      "        \"\"\"\n",
      "        pass\n",
      "    \n",
      "    \n",
      "        \n",
      "class Layer():\n",
      "    \"\"\"\n",
      "    Create a feedforward layer with identity activation\n",
      "    \"\"\"\n",
      "    def __init__(self, input_size = 10, output_size = None, dtype=REAL):\n",
      "        self.dtype = dtype\n",
      "        self._weight_matrix = None\n",
      "        self._bias_units = None\n",
      "        self.input_size = input_size\n",
      "        self.output_size = output_size\n",
      "        \n",
      "        self._forward_layers = []\n",
      "        self._backward_layers = []\n",
      "        \n",
      "        self._activation = None\n",
      "        self.dEdy = None\n",
      "        self.dEdz = None\n",
      "        \n",
      "        self._weight_diff = None\n",
      "        self._bias_weight_diff = None\n",
      "        \n",
      "        if self.input_size is not None and self.output_size is not None:\n",
      "            self.create_weights()\n",
      "        \n",
      "    def activate_forward_layers(self):\n",
      "        for layer in self._forward_layers:\n",
      "            layer.activate(self._activation)\n",
      "        \n",
      "    def activate(self, input):\n",
      "        # run net forward using input\n",
      "        self.forward_propagate(input)\n",
      "        # transfer activation as input to next layers:\n",
      "        self.activate_forward_layers()\n",
      "        \n",
      "    def backpropagate(self, signal):\n",
      "        \"\"\"\n",
      "        Get local error responsability using\n",
      "        the derivative of error with respect\n",
      "        to output times the derivative of the\n",
      "        local parameters dy / dz\n",
      "        \"\"\"\n",
      "        # signal backwards is given by taking weight matrix\n",
      "        # with signal with derivative\n",
      "        self.dEdy = signal * self.dydz()\n",
      "        \n",
      "        self.dEdz = np.dot(self.dEdy, self._weight_matrix)\n",
      "        \n",
      "        # updates to weight matrix are given by outer\n",
      "        # product of signal with input:\n",
      "        \n",
      "        self._weight_diff = vector_outer_product(self.dEdy, self.layer_input())\n",
      "        \n",
      "        # updates to bias units are given by signal\n",
      "        self._bias_weight_diff = signal.T\n",
      "        \n",
      "        for layer in self._backward_layers:\n",
      "            layer.backpropagate(self.dEdz)\n",
      "        \n",
      "    def layer_input(self):\n",
      "        \"\"\"\n",
      "        Input is sum of activations of backward\n",
      "        layers.\n",
      "        \"\"\"\n",
      "        if len(self._backward_layers) > 1:\n",
      "            return sum(layer._activation for layer in self._backward_layers)\n",
      "        else:\n",
      "            return self._backward_layers[0]._activation\n",
      "        \n",
      "    def error_activate(self, target):\n",
      "        \"\"\"\n",
      "        Start the backpropagation using a target\n",
      "        by getting the initial error responsability\n",
      "        as dE / dz.\n",
      "        \n",
      "        dEdy is then provided for the backward layers\n",
      "        iteratively.\n",
      "        \"\"\"\n",
      "        \n",
      "        # get the error here\n",
      "        self.backpropagate(self._activation - target)\n",
      "        \n",
      "    def clear(self):\n",
      "        \"\"\"\n",
      "        Clears the activation and the local\n",
      "        error responsibility for this layer\n",
      "        \"\"\"\n",
      "        self._activation       = None\n",
      "        self.dEdy              = None\n",
      "        self.dEdz              = None\n",
      "        self._weight_diff      = None\n",
      "        self._bias_weight_diff = None\n",
      "        \n",
      "    def create_weights(self):\n",
      "        \"\"\"\n",
      "        Randomly initialize the weights for this layer\n",
      "        with gaussian noise with std 1 / input size\n",
      "        \"\"\"\n",
      "        self._weight_matrix = (\n",
      "            (1. / self.input_size) *\n",
      "            np.random.randn(\n",
      "                self.output_size,\n",
      "                self.input_size)\n",
      "        ).astype(self.dtype)\n",
      "        \n",
      "        self._bias_units = (\n",
      "            (1. / self.input_size) *\n",
      "            np.random.randn(self.output_size)\n",
      "        ).astype(self.dtype)\n",
      "        \n",
      "    def activation_function(self, x):\n",
      "        \"\"\"Linear\"\"\"\n",
      "        return x\n",
      "    \n",
      "    def dydz(self):\n",
      "        return 1.0\n",
      "    \n",
      "    def error(self, target):\n",
      "        \"\"\"\n",
      "        Mean squared error\n",
      "        \"\"\"\n",
      "        return 0.5 * (target - self._activation)**2\n",
      "        \n",
      "    def forward_propagate(self, input):\n",
      "        \"\"\"\n",
      "        use the weights and the activation function\n",
      "        to react to the input\n",
      "        \n",
      "        TODO: use the `out' parameter of numpy dot to\n",
      "        gain speed on memory allocation.\n",
      "        \n",
      "        Inputs\n",
      "        ------\n",
      "        \n",
      "        inputs ndarray : the input data\n",
      "        \n",
      "        Outputs\n",
      "        -------\n",
      "        \n",
      "        activation ndarray : the activation for this input\n",
      "        \n",
      "        \"\"\"\n",
      "        self._activation = self.activation_function( np.dot(input, self._weight_matrix.T) + self._bias_units)\n",
      "        return self._activation\n",
      "    \n",
      "    def _connect_layer(self, layer):\n",
      "        \"\"\"\n",
      "        Adds the layer to the forward and\n",
      "        backward lists of layers to connect\n",
      "        the graph.\n",
      "        \"\"\"\n",
      "        self._forward_layers.append(layer)\n",
      "        layer.add_backward_layer(self)\n",
      "        \n",
      "    def add_backward_layer(self, layer):\n",
      "        \"\"\"\n",
      "        Connect a layer to the antecedents\n",
      "        of this layer in the graph.\n",
      "        \"\"\"\n",
      "        self._backward_layers.append(layer)\n",
      "        \n",
      "    def connect_to(self, layer):\n",
      "        \"\"\"\n",
      "        Connect two layers together.\n",
      "        \"\"\"\n",
      "        if self.output_size == None:\n",
      "            self.output_size = layer.input_size\n",
      "            self.create_weights()\n",
      "            self._connect_layer(layer)\n",
      "        elif layer.input_size == self.output_size:\n",
      "            self._connect_layer(layer)\n",
      "        else:\n",
      "            raise BaseException(\"Current layer's output size does not match input size for next layer\")\n",
      "            \n",
      "    def __repr__(self):\n",
      "        return \"<\" + self.__class__.__name__ + \" \" + str({\"activation\": self.activation_function.__doc__, \"input_size\": self.input_size, \"output_size\": self.output_size})+\">\"\n",
      "  \n",
      "class LogisticLayer(Layer):\n",
      "    \n",
      "    def dydz(self):\n",
      "        return self._activation - (self._activation)**2\n",
      "    \n",
      "    def error(self, target):\n",
      "        \"\"\"\n",
      "        Cross entropy error\n",
      "        \"\"\"\n",
      "        return -(target * np.log(self._activation) + (1.0 - target) * np.log(1.0 - self._activation))\n",
      "    \n",
      "    def activation_function(self, x):\n",
      "        \"\"\"Sigmoid\"\"\"\n",
      "        return sigmoid(x)\n",
      "    \n",
      "class TanhLayer(Layer):\n",
      "    \"\"\"\n",
      "    Neural net layer with Hyperbolic tangent nonlinearity\n",
      "    \"\"\"\n",
      "    \n",
      "    def activation_function(self, x):\n",
      "        \"\"\"Tanh\"\"\"\n",
      "        return np.tanh(x)\n",
      "    \n",
      "    def dydz(self):\n",
      "        \"\"\"\n",
      "        Derivative of hyperbolic tangent\n",
      "        \"\"\"\n",
      "        return 1.0 - self._activation**2\n",
      "    \n",
      "    def error(self, target):\n",
      "        \"\"\"\n",
      "        Cross entropy error (we reshape tanh activation into\n",
      "        a sigmoid like activation and then apply cross entropy\n",
      "        criterion to it)\n",
      "        \n",
      "        \"\"\"\n",
      "        resized_activation = self._activation + 1.0 / 2.0\n",
      "        return -(target * np.log(resized_activation) + (1.0 - target) * np.log(1.0 - resized_activation))\n",
      "    \n",
      "class Trainer():\n",
      "    def __init__(self, network):\n",
      "        # should freeze the structure of the network or have\n",
      "        # robust method of linking to the elements inside\n",
      "        self.network = network\n",
      "        self.parameters = \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_net():\n",
      "    \n",
      "    # create a test dataset\n",
      "    xor_dataset, xor_labels = (np.array([\n",
      "            [0, 0],\n",
      "            [0, 1],\n",
      "            [1, 0],\n",
      "            [1, 1]], dtype=REAL),\n",
      "                               np.array([\n",
      "            [0, 1],\n",
      "            [1, 0],\n",
      "            [1, 0],\n",
      "            [0, 1]], dtype=REAL))\n",
      "    \n",
      "    \n",
      "    # create a small network:\n",
      "\n",
      "    net = Network()\n",
      "    first_layer = LogisticLayer(xor_dataset.shape[1])\n",
      "    second_layer = LogisticLayer(6, xor_labels.shape[1])\n",
      "\n",
      "    first_layer.connect_to(second_layer)\n",
      "    net.add_layer(first_layer, input=True)\n",
      "    net.add_layer(second_layer, output=True)\n",
      "    \n",
      "    net.clear()\n",
      "    net.activate(xor_dataset)\n",
      "    net.backpropagate(xor_labels)\n",
      "\n",
      "    for layer in net._layers:\n",
      "        assert(layer._weight_matrix.shape == layer._weight_diff.mean(axis=-1).shape), \"Weight updates are not the same size\"\n",
      "        assert(layer._bias_units.shape == layer._bias_weight_diff.mean(axis=-1).shape), \"Bias updates are not the same size\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_net()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}